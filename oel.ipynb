import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, roc_auc_score

# Load the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data"
names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']
heart_data = pd.read_csv(url, names=names)

# Data preprocessing and exploration
# Replace missing values denoted by '?' with NaN and drop NaN rows
heart_data = heart_data.replace('?', np.nan)
heart_data = heart_data.dropna()

# Convert some columns to appropriate data types
to_convert = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
heart_data.loc[:, to_convert] = heart_data[to_convert].astype(float)


# Convert categorical variables to dummy/indicator variables
cp = pd.get_dummies(heart_data['cp'], prefix='cp', drop_first=True)
restecg = pd.get_dummies(heart_data['restecg'], prefix='restecg', drop_first=True)
slope = pd.get_dummies(heart_data['slope'], prefix='slope', drop_first=True)
thal = pd.get_dummies(heart_data['thal'], prefix='thal', drop_first=True)
heart_data = pd.concat([heart_data, cp, restecg, slope, thal], axis=1)
heart_data.drop(['cp', 'restecg', 'slope', 'thal'], axis=1, inplace=True)

# Mapping target variable: num (presence of heart disease)
heart_data['target'] = heart_data['num'].apply(lambda x: 1 if x > 0 else 0)
heart_data.drop('num', axis=1, inplace=True)

# Separate features and target variable
X = heart_data.drop('target', axis=1)
y = heart_data['target']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardize features by removing the mean and scaling to unit variance
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Visualizations for EDA
plt.figure(figsize=(14, 7))

# Histograms of numerical variables
plt.subplot(2, 3, 1)
plt.hist(heart_data['age'], bins=20, edgecolor='black')
plt.title('Distribution of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')

plt.subplot(2, 3, 2)
plt.hist(heart_data['trestbps'], bins=20, edgecolor='black')
plt.title('Distribution of Resting Blood Pressure (trestbps)')
plt.xlabel('Resting Blood Pressure')
plt.ylabel('Frequency')

plt.subplot(2, 3, 3)
plt.hist(heart_data['chol'], bins=20, edgecolor='black')
plt.title('Distribution of Serum Cholesterol (chol)')
plt.xlabel('Serum Cholesterol')
plt.ylabel('Frequency')

plt.subplot(2, 3, 4)
plt.hist(heart_data['thalach'], bins=20, edgecolor='black')
plt.title('Distribution of Maximum Heart Rate Achieved (thalach)')
plt.xlabel('Maximum Heart Rate')
plt.ylabel('Frequency')

plt.subplot(2, 3, 5)
plt.hist(heart_data['oldpeak'], bins=20, edgecolor='black')
plt.title('Distribution of ST Depression Induced by Exercise (oldpeak)')
plt.xlabel('ST Depression')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Box plots for numerical variables
plt.figure(figsize=(14, 7))

plt.subplot(2, 3, 1)
sns.boxplot(x='target', y='age', data=heart_data)
plt.title('Age vs Target')

plt.subplot(2, 3, 2)
sns.boxplot(x='target', y='trestbps', data=heart_data)
plt.title('Resting Blood Pressure vs Target')

plt.subplot(2, 3, 3)
sns.boxplot(x='target', y='chol', data=heart_data)
plt.title('Serum Cholesterol vs Target')

plt.subplot(2, 3, 4)
sns.boxplot(x='target', y='thalach', data=heart_data)
plt.title('Maximum Heart Rate vs Target')

plt.subplot(2, 3, 5)
sns.boxplot(x='target', y='oldpeak', data=heart_data)
plt.title('ST Depression vs Target')

plt.tight_layout()
plt.show()

# Correlation matrix
plt.figure(figsize=(10, 8))
corr = heart_data.corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Model Training and Evaluation

# Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_scaled, y_train)
lr_pred = lr.predict(X_test_scaled)
lr_acc = accuracy_score(y_test, lr_pred)
lr_cm = confusion_matrix(y_test, lr_pred)
lr_cr = classification_report(y_test, lr_pred)

# Random Forest Classifier
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train_scaled, y_train)
rf_pred = rf.predict(X_test_scaled)
rf_acc = accuracy_score(y_test, rf_pred)
rf_cm = confusion_matrix(y_test, rf_pred)
rf_cr = classification_report(y_test, rf_pred)

# Support Vector Classifier (SVC)
svc = SVC(probability=True)
svc.fit(X_train_scaled, y_train)
svc_pred = svc.predict(X_test_scaled)
svc_acc = accuracy_score(y_test, svc_pred)
svc_cm = confusion_matrix(y_test, svc_pred)
svc_cr = classification_report(y_test, svc_pred)

# ROC Curve and AUC for each model
plt.figure(figsize=(14, 7))

# Logistic Regression
lr_probs = lr.predict_proba(X_test_scaled)[:, 1]
lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_probs)
lr_auc = roc_auc_score(y_test, lr_probs)
plt.plot(lr_fpr, lr_tpr, linestyle='-', label=f'Logistic Regression (AUC = {lr_auc:.2f})')

# Random Forest Classifier
rf_probs = rf.predict_proba(X_test_scaled)[:, 1]
rf_fpr, rf_tpr, _ = roc_curve(y_test, rf_probs)
rf_auc = roc_auc_score(y_test, rf_probs)
plt.plot(rf_fpr, rf_tpr, linestyle='--', label=f'Random Forest (AUC = {rf_auc:.2f})')

# Support Vector Classifier
svc_probs = svc.predict_proba(X_test_scaled)[:, 1]
svc_fpr, svc_tpr, _ = roc_curve(y_test, svc_probs)
svc_auc = roc_auc_score(y_test, svc_probs)
plt.plot(svc_fpr, svc_tpr, linestyle='-.', label=f'Support Vector Classifier (AUC = {svc_auc:.2f})')

# Plotting the ROC curve for a purely random classifier
plt.plot([0, 1], [0, 1], linestyle=':', color='black', label='Random Classifier (AUC = 0.50)')

# Title and labels
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')

plt.show()

# Compare the models
models = ['Logistic Regression', 'Random Forest', 'Support Vector Classifier']
accuracies = [lr_acc, rf_acc, svc_acc]
confusion_matrices = [lr_cm, rf_cm, svc_cm]
classification_reports = [lr_cr, rf_cr, svc_cr]

for i, model in enumerate(models):
    print(f"Model: {model}")
    print(f"Accuracy: {accuracies[i]:.4f}")
    print("Confusion Matrix:")
    print(confusion_matrices[i])
    print("Classification Report:")
    print(classification_reports[i])
    print("\n")

# Select the best performing model based on accuracy
best_model_idx = np.argmax(accuracies)
best_model = models[best_model_idx]

print(f"Best Performing Model: {best_model} with Accuracy: {accuracies[best_model_idx]:.4f}")
